{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "# Thamanya AI Discovery - Data Pipeline (Zero-Dollar)\n",
        "\n",
        "This notebook handles:\n",
        "1.  Downloading audio from YouTube video URL.\n",
        "2.  Transcribing using OpenAI Whisper (Free GPU on Colab).\n",
        "3.  Chunking text into meaningful segments (300-500 chars).\n",
        "4.  Generating Embeddings using Gemini API.\n",
        "5.  Upserting data to Supabase (pgvector)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install transport"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/openai/whisper.git\n",
        "!pip install -q yt-dlp\n",
        "!pip install -q google-generativeai\n",
        "!pip install -q supabase\n",
        "!pip install -q pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup"
      },
      "outputs": [],
      "source": [
        "import whisper\n",
        "import yt_dlp\n",
        "import google.generativeai as genai\n",
        "from supabase import create_client, Client\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# Make sure to set these secrets in Colab (Keys -> Add new)\n",
        "# SUPABASE_URL, SUPABASE_KEY, GEMINI_API_KEY\n",
        "\n",
        "try:\n",
        "    SUPABASE_URL = userdata.get('SUPABASE_URL')\n",
        "    SUPABASE_KEY = userdata.get('SUPABASE_KEY')\n",
        "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "except:\n",
        "    print(\"Secrets not found. Please set SUPABASE_URL, SUPABASE_KEY, GEMINI_API_KEY in Colab secrets.\")\n",
        "    # Placeholder for local testing if needed\n",
        "    SUPABASE_URL = \"\"\n",
        "    SUPABASE_KEY = \"\"\n",
        "    GEMINI_API_KEY = \"\"\n",
        "\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
        "\n",
        "print(\"Libraries loaded & Configured\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step1_desc"
      },
      "source": [
        "## 1. Download Audio from YouTube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_audio"
      },
      "outputs": [],
      "source": [
        "def download_audio(url, output_filename=\"audio.mp3\"):\n",
        "    ydl_opts = {\n",
        "        'format': 'bestaudio/best',\n",
        "        'postprocessors': [{\n",
        "            'key': 'FFmpegExtractAudio',\n",
        "            'preferredcodec': 'mp3',\n",
        "            'preferredquality': '192',\n",
        "        }],\n",
        "        'outtmpl': output_filename.replace('.mp3', ''),\n",
        "        'quiet': True\n",
        "    }\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        info = ydl.extract_info(url, download=True)\n",
        "        title = info.get('title', 'Unknown Title')\n",
        "        print(f\"Downloaded: {title}\")\n",
        "        return title\n",
        "\n",
        "# Example Usage\n",
        "VIDEO_URL = \"https://www.youtube.com/watch?v=VIDEO_ID_HERE\" # REPLACE ME\n",
        "EPISODE_TITLE = download_audio(VIDEO_URL, \"episode.mp3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step2_desc"
      },
      "source": [
        "## 2. Transcribe with Whisper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "transcribe"
      },
      "outputs": [],
      "source": [
        "model = whisper.load_model(\"large\") # Use 'medium' if 'large' is too slow or OOM\n",
        "result = model.transcribe(\"episode.mp3\")\n",
        "segments = result['segments']\n",
        "print(f\"Transcribed {len(segments)} segments.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step3_desc"
      },
      "source": [
        "## 3. Chunking & Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chunk_embed"
      },
      "outputs": [],
      "source": [
        "def chunk_segments(segments, max_chars=500):\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "    start_time = 0\n",
        "    \n",
        "    for seg in segments:\n",
        "        text = seg['text'].strip()\n",
        "        if not text: continue\n",
        "        \n",
        "        if not current_chunk:\n",
        "            start_time = seg['start']\n",
        "            \n",
        "        if len(current_chunk) + len(text) + 1 <= max_chars:\n",
        "            current_chunk += \" \" + text\n",
        "            end_time = seg['end']\n",
        "        else:\n",
        "            # Push chunk\n",
        "            chunks.append({\n",
        "                'content': current_chunk.strip(),\n",
        "                'start_time': start_time,\n",
        "                'end_time': end_time\n",
        "            })\n",
        "            # Start new chunk\n",
        "            current_chunk = text\n",
        "            start_time = seg['start']\n",
        "            end_time = seg['end']\n",
        "            \n",
        "    # Last chunk\n",
        "    if current_chunk:\n",
        "        chunks.append({\n",
        "            'content': current_chunk.strip(),\n",
        "            'start_time': start_time,\n",
        "            'end_time': end_time\n",
        "        })\n",
        "    return chunks\n",
        "\n",
        "chunks = chunk_segments(segments)\n",
        "print(f\"Created {len(chunks)} chunks.\")\n",
        "\n",
        "def get_embedding(text):\n",
        "    # text-embedding-004 is recommended for lower cost/high perf\n",
        "    result = genai.embed_content(\n",
        "        model=\"models/text-embedding-004\",\n",
        "        content=text,\n",
        "        task_type=\"retrieval_document\",\n",
        "        title=\"Podcast Segment\"\n",
        "    )\n",
        "    return result['embedding']\n",
        "\n",
        "# Process chunks (Batch this in production to avoid rate limits, but for POC sequential is fine or small batches)\n",
        "import time\n",
        "processed_chunks = []\n",
        "for i, chunk in enumerate(chunks):\n",
        "    chunk['embedding'] = get_embedding(chunk['content'])\n",
        "    processed_chunks.append(chunk)\n",
        "    if i % 10 == 0: \n",
        "        print(f\"Encoded {i}/{len(chunks)}\")\n",
        "        time.sleep(0.5) # simple rate limit handling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step4_desc"
      },
      "source": [
        "## 4. Upsert to Supabase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upsert"
      },
      "outputs": [],
      "source": [
        "# First, insert episode if not exists\n",
        "episode_data = {\n",
        "    'title': EPISODE_TITLE,\n",
        "    'url': VIDEO_URL,\n",
        "    'published_at': 'now()', # Or extract from metadata\n",
        "    'metadata': {}\n",
        "}\n",
        "\n",
        "res = supabase.table('episodes').insert(episode_data).execute()\n",
        "episode_id = res.data[0]['id']\n",
        "print(f\"Inserted Episode ID: {episode_id}\")\n",
        "\n",
        "# Prepare chunk data\n",
        "db_rows = []\n",
        "for chunk in processed_chunks:\n",
        "    db_rows.append({\n",
        "        'episode_id': episode_id,\n",
        "        'content': chunk['content'],\n",
        "        'start_time': chunk['start_time'],\n",
        "        'end_time': chunk['end_time'],\n",
        "        'embedding': chunk['embedding']\n",
        "    })\n",
        "\n",
        "# Bulk insert\n",
        "try:\n",
        "    res = supabase.table('chunks').insert(db_rows).execute()\n",
        "    print(\"Inserted chunks successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error inserting chunks: {e}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
